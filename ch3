#自動開啟網頁

import webbrowser

address = input('請輸入地址')
webbrowser.open('https://www.google.com.tw/maps/place/' + address)

-------------------------------------------------------------------------------
#requests使用

import requests

url = 'https://www.google.com.tw'  
response = requests.get(url)              #requests.get()之後傳回的資料型態是Response物件
print(type(response))

if response.status_code == requests.codes.ok:
    print('取得網頁成功')
    print('網址訊息 :',response.url)
    print('表頭訊息 :',response.headers)
    print('cookie訊息 :',response.cookies)
else:
    print('取得網頁內容失敗')

-------------------------------------------------------------------------------
#選取某字，計算出現的次數

import requests
import re

url = 'https://www.juksy.com/article/120091-NBA%EF%BC%8F%E5%8B%87%E5%A3%AB'
response = requests.get(url)
if response.status_code == requests.codes.ok:
    pattern = input('輸入想輸入的文字:')   

    if pattern in response.text:
        print('搜尋 %s 成功' %pattern)
    else:
        print('沒有找到 %s'%pattern)
    #計算出現次數
    name = re.findall(pattern,response.text)
    if name != None:
        print('%s 出現 %d 次'%(pattern,len(name)))
    else:
        print('%s 出現0次' %pattern)
else:
    print('網頁下載失敗')

-------------------------------------------------------------------------------------
#測試網頁下載是否成功
import requests

url = 'https//google.com.tw/Ooo'
try:
    response = requests.get(url)
    response.raise_for_status()
    print('下載成功')
except Exception as err:
    print('網頁下載失敗: %s'%err)
print('程式結束')


------------------------------------------------------------------------------------
#爬蟲程式偽裝成瀏覽器
import requests

#先在前端加入表頭(headers)內容，是字典的形式，可以偽裝成瀏覽器，按F12去Network中找，複製貼上改成字典格式即可
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 \
          Safari/537.36'}


url = 'https://www.kingstone.com.tw/basic/2015630743935/?zone=book&lid=home_act_prod2'
response = requests.get(url,headers=headers)
response.raise_for_status()
print('偽裝瀏覽器擷取網路資料成功')


------------------------------------------------------------------------------------
# urllib模組使用
import urllib.request

url = 'https://google.com.tw'
response = urllib.request.urlopen(url,timeout=20)    #獲取網頁HTTPResponse物件，並預設20秒，超時就跳出異常
print(type(response))
print(response)
print(response.read().decode('big5'))  #讀取物件並轉成'big5'碼

print('版本:',response.version)          #版本編號
print('網址:',response.geturl())         #物件的網址
print('下載:',response.status)           #下載狀況，如果回傳200則正常
print('表頭:')                           #取得表頭內容
for i in response.getheaders():
    print(i)
    
   
-------------------------------------------------------------------------------------
#列印所拜訪網頁的HTML文件
import urllib.request


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 \
          Safari/537.36'}


url = 'https://www.kingstone.com.tw/basic/2015630743935/?zone=book&lid=home_act_prod2'
req = urllib.request.Request(url,headers=headers)
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))

-------------------------------------------------------------------------------------
#使用urllib.request模組的urlretrieve()下載圖片
import urllib.request
url_pict = 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/TSMC-Logo.svg/330px-TSMC-Logo.svg.png'
fn = 'TSMC.png'
pict = urllib.request.urlretrieve(url_pict,fn)   #圖片檔將會下載到此程式碼所在的資料夾中


-------------------------------------------------------------------------------------
#URL編碼與中文編碼轉換

from urllib import parse

s = '台灣積體電路製造'
url_code = parse.quote(s)
print('URL編碼 : ',url_code)      #quote()將中文轉成URL編碼
code = parse.unquote(url_code)    #unquote()將URL編碼轉成中文
print('中文編碼 : ',code)


--------------------------------------------------------------------------------------
#URL分析拆解
from urllib import parse

url = 'https://docs.python.org/3/search.html?q=parse&check_keywords=yes&area=default'
urp = parse.urlparse(url)
print(type(urp))
print(urp)
print('scheme = ',urp.scheme)      #URL所使用的方案
print('netloc = ',urp.netloc)      #網路位置
print('path = ',urp.path)          #分層路徑 
print('params = ',urp.params)      #最後路徑元素參數
print('query = ',urp.query) 
print('fragment = ',urp.fragment)  #片段標示符號

----------------------------------------------------------------------------------------
#URL合成
from urllib import parse

scheme = 'https'
netloc =  'docs.python.org'
path =  '/3/search.html'
params = ''
query =  'q=parse&check_keywords=yes&area=default'
fragment = ''
url_unparse = parse.urlunparse((scheme,netloc,path,params,query,fragment))   #反向工作
print(url_unparse)

url_unsplit = parse.urlunsplit((scheme,netloc,path,query,fragment))　　　　　　#反向工作
print(url_unsplit)


----------------------------------------------------------------------------------------
#字典格式資料轉成網頁字串
from urllib import parse

url_python = 'https://docs.python.org/3/search.html?'

query = {
    'q':'parse',
    'check_keywords':'yes',
    'area':'default'
    }
url = url_python + parse.urlencode(query)
print(url)

-----------------------------------------------------------------------------------------
from urllib import parse
query_str = 'q=parse&check_keywords=yes&area=default'

print('parse.parse_qs = ',parse.parse_qs(query_str))      #將網址查詢字串轉成字典
print('parse.parse_qsl = ',parse.parse_qsl(query_str))    #將網址查詢字串轉成串列

-----------------------------------------------------------------------------------------
#異常處理
from urllib import request,error

header = {'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
          AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0\
          Safari/537.36"}

#錯誤1
url_error = 'https://24h.pchome.comm.tw/'             #錯誤網址
try:
    htmlfile = request.urlopen(url_error)
except error.HTTPError as e:
    print('錯誤代碼 : ',e.code)
    print('錯誤原因 : ',e.reason)
    print('回應表頭 : ',e.headers)
except error.URLError as e:
    print('錯誤原因 : ',e.reason)
else:
    print('擷取網路資料成功')

#錯誤2
url = 'https://24h.pchome.com.tw/' 
try:
    htmlfile = request.urlopen(url)
except error.HTTPError as e:
    print('錯誤代碼 : ',e.code)
    print('錯誤原因 : ',e.reason)
    print('回應表頭 : ',e.headers)
except error.URLError as e:
    print('錯誤原因 : ',e.reason)
else:
    print('擷取網路資料成功')

#正確
url = 'https://24h.pchome.com.tw/'                  #網址正確
try:
    req = request.Request(url,headers=header)
    htmlfile = request.urlopen(req)
except error.URLError as e:
    print('錯誤原因 : ',e.reason)
else:
    print('網頁擷取成功')

---------------------------------------------------------------------------------
# HTTP回應
import requests

headers = {'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
          AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0\
          Safari/537.36"}

url = 'https://www.httpbin.org/post'
form_data = {'gender':'M','page':'1'}
r = requests.post(url,json = form_data,headers= headers)
print(r.url)

print('r.request.headers : \n',r.request.headers)   

print('r.headers : \n',r.headers)

print(r.status_code)            #回應200，則是正常 
print(r.reason)                 #正常回應是OK
print(r.encoding)               #列出伺服器回應內文的編碼方式
print(r.text)                   #獲得HTML內容

--------------------------------------------------------------------------------------
import requests

url = r'https://www.httpbin.org/response-headers?freeform='
r = requests.get(url)
if r.status_code == 200:
    print(r.headers.get('content-type'))
    print(r.json())    #將json數據轉python

#取得圖片並儲存
url = r'https://www.httpbin.org/image/jpeg'
r = requests.get(url)
img = r.content

fn = '3-7.jpg'
with open(fn,'wb') as fout:
    fout.write(img)
-----------------------------------------------------------------------------------------
#登入cookie
import requests

url = r'http://httpbin.org/cookies'
cookies = dict(key1='value1')
r = requests.get(url,cookies=cookies)
print(r.text)


---------------------------------------------------------------------------------------
#設定代理IP
import requests

proxies = {'http':'http://85.45.92.167:8080'}

r = requests.get('https://docs.python.org',proxies = proxies)
if r.status_code == 200:
    print('代理IP使用成功')
    
    
